{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓取博主主页\n",
    "### 安装weibo_spider\n",
    "```bash\n",
    "python3 -m pip install weibo_spider\n",
    "```\n",
    "### 抓取博主主页内容\n",
    "*注：weibo为输出目录，config.json为配置文件，可以设置抓取内容*\n",
    "```bash\n",
    "python3 -m weibo_spider --output_dir=\"data\" --config_path=\"config.json\"\n",
    "```\n",
    "*配置文件示例*\n",
    "```json\n",
    "{\n",
    "    \"user_id_list\": [\"user id\"],\n",
    "    \"filter\": 1,\n",
    "    \"since_date\": \"2021-01-01\",\n",
    "    \"end_date\": \"now\",\n",
    "    \"random_wait_pages\": [1, 5],\n",
    "    \"random_wait_seconds\": [6, 10],\n",
    "    \"global_wait\": [[1000, 3600], [500, 2000]],    \n",
    "    \"write_mode\": [\"csv\",\"txt\"],\n",
    "    \"pic_download\": 0,\n",
    "    \"video_download\": 0,\n",
    "    \"result_dir_name\": 0,\n",
    "    \"cookie\": \"your cookie\",\n",
    "    \"sqlite_config\": \"weibo.db\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取抓取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/少数派sspai/1914010467.csv\")\n",
    "article=df[\"微博正文\"]\n",
    "likes=df[\"点赞数\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成帖子词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import PIL .Image as image\n",
    "text = \" \".join(i for i in article)\n",
    "wordcloud=WordCloud(font_path=\"data/msyh.ttc\").generate(text)\n",
    "word_image=wordcloud.to_image()\n",
    "word_image.save('data/convert.png','png')\n",
    "# 显示图片\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "lena = mpimg.imread('data/convert.png')\n",
    "plt.imshow(lena)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 帖子热度统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入字体管理库\n",
    "from matplotlib import font_manager\n",
    "likes=df.loc[:,\"点赞数\"]\n",
    "comment=df.loc[:,\"评论数\"]\n",
    "forward=df.loc[:,\"转发数\"]\n",
    "time=df.loc[:,\"发布时间\"]\n",
    "my_font = font_manager.FontProperties(fname=r\"data/msyh.ttc\")\n",
    "plt.figure(figsize=(15,8),dpi=80)\n",
    "plt.plot(range(len(likes)), likes, label='点赞数',color=\"r\")\n",
    "plt.plot(range(len(comment)), comment, label='评论数',color=\"b\")\n",
    "plt.plot(range(len(forward)), forward, label='转发数',color=\"y\")\n",
    "# 设置x，y坐标\n",
    "plt.xticks(time)\n",
    "plt.yticks(range(500))\n",
    "# 设置网格线\n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(prop=my_font,loc=\"upper left\")\n",
    "plt.title('微博热度折线图',fontproperties=my_font)\n",
    "plt.show()\n",
    "plt.savefig('data/hot.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取微博热搜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import schedule\n",
    "import time\n",
    "import csv\n",
    "from xlutils.copy import copy\n",
    "url = 'https://s.weibo.com/top/summary?Refer=top_hot&topnav=1&wvr=6'\n",
    "with open('data/微博热搜.csv', 'a+', newline='') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(['时间', '排名', '热度', '内容'])\n",
    "def run():\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36',\n",
    "        'Cookie': 'WEIBOCN_WM=3349; MLOGIN=0; loginScene=102003; SUB=_2A25OZUb0DeRhGeFM71cR8SjPyTyIHXVtpmq8rDV6PUJbkdANLUTtkW1NQNrR0hDYvbkyXXHvhSXTK6gLZwPIvqhj; _T_WM=2f61b9622f544de949daf22f40ebcc99'\n",
    "    }\n",
    "    cookie = {\n",
    "        'Cookie': ''\n",
    "    }\n",
    "    response = requests.get(url, headers=header, cookies=cookie)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    items = soup.find_all('td', class_='td-02')\n",
    "    time_stamp = time.strftime('%Y/%m/%d %H:%M', time.localtime(time.time()))  # 时间戳\n",
    "\n",
    "    for i, item in enumerate(items[1:11]):\n",
    "        result = []\n",
    "        rank = '第{0}名'.format(i+1)     # 微博排名\n",
    "        num = str(item.find('span')).replace('<span>', '').replace('</span>', '')  # 微博热度\n",
    "        title = item.find('a').text  # 微博内容\n",
    "        result.append(time_stamp)\n",
    "        result.append(rank)\n",
    "        result.append(num)\n",
    "        result.append(title)\n",
    "        with open('data/微博热搜.csv', 'a+',newline='') as f:\n",
    "            f_csv = csv.writer(f)\n",
    "            f_csv.writerow(result)\n",
    "    print(time_stamp)\n",
    "run()\n",
    "schedule.every(30).seconds.do(run)\n",
    "while True:\n",
    "    schedule.run_pending()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态热搜图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import options as opts\n",
    "from pyecharts.charts import Bar, Timeline,Grid\n",
    "from pyecharts.globals import ThemeType\n",
    "from pyecharts.commons.utils import JsCode\n",
    "import pandas as pd\n",
    "data=pd.read_csv('data/微博热搜.csv')\n",
    "tl = Timeline({\"theme\": ThemeType.MACARONS})\n",
    "for i in range(4):\n",
    "    l=list(data['时间'])[i*10]\n",
    "    print(l)\n",
    "    bar = (\n",
    "        Bar({\"theme\": ThemeType.MACARONS})\n",
    "        .add_xaxis(list(data['内容'])[i*10:i*10+10][::-1])\n",
    "        .add_yaxis(\"微博热搜榜\", list(data['热度'])[i*10:i*10+10][::-1])\n",
    "        .reversal_axis()\n",
    "        .set_global_opts(\n",
    "            title_opts=opts.TitleOpts(\"{}\".format(list(data['时间'])[i*10]),pos_right='0%',pos_bottom='15%'),\n",
    "            xaxis_opts=opts.AxisOpts(\n",
    "                splitline_opts=opts.SplitLineOpts(is_show=True),\n",
    "                position='top',\n",
    "                name_gap=10,\n",
    "                boundary_gap=['20%', '20%']),\n",
    "            yaxis_opts=opts.AxisOpts(splitline_opts=opts.SplitLineOpts(is_show=True),\n",
    "                                     axislabel_opts=opts.LabelOpts(color='#FF7F50')),)\n",
    "        .set_series_opts(label_opts=opts.LabelOpts(position=\"right\",color='#9400D3'))\n",
    "    )\n",
    "    grid = (\n",
    "        Grid()\n",
    "        .add(bar, grid_opts=opts.GridOpts(pos_left=\"25%\",pos_right=\"0%\"))\n",
    "    )\n",
    "    tl.add(grid, \"{}分\".format(i))\n",
    "    tl.add_schema(\n",
    "        play_interval=1000,   #播放速度\n",
    "        is_timeline_show=False,  #是否显示 timeline 组件\n",
    "        is_auto_play=True,\n",
    "    )\n",
    "tl.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成热搜词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "from wordcloud import WordCloud\n",
    "import PIL .Image as image\n",
    "import imageio\n",
    "\n",
    "\n",
    "hots=data['内容']\n",
    "text = \" \".join(i for i in hots)\n",
    "wordcloud=WordCloud(font_path=\"data/msyh.ttc\",\n",
    "                    width=1000,\n",
    "                    height=700,\n",
    "                    background_color='white').generate(text)\n",
    "word_image=wordcloud.to_image()\n",
    "word_image.save('data/hots.png','png')\n",
    "# 显示图片\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "lena = mpimg.imread('data/hots.png')\n",
    "plt.imshow(lena)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按关键词搜索内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re \n",
    "import os\n",
    "import urllib.parse\n",
    "import time\n",
    "import pandas as pd\n",
    "#header文件     \n",
    "headers ={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0)',\n",
    "    'Cookie': 'WEIBOCN_WM=3349; MLOGIN=0; loginScene=102003; SUB=_2A25OZUb0DeRhGeFM71cR8SjPyTyIHXVtpmq8rDV6PUJbkdANLUTtkW1NQNrR0hDYvbkyXXHvhSXTK6gLZwPIvqhj; _T_WM=2f61b9622f544de949daf22f40ebcc99'\n",
    "}\n",
    "#搜索名词\n",
    "keyword = 'iphone'\n",
    "#创建同名文件夹\n",
    "filepath = str(r'data/'+keyword)\n",
    "if not os.path.exists(filepath):\n",
    "    os.mkdir(filepath)\n",
    "#搜索名词下40页源码并保存为txt文件\n",
    "kw=urllib.parse.quote(keyword)\n",
    "s_url ='https://s.weibo.com/weibo?q='+kw+'&wvr=6&b=1&Refer=SWeibo_box'\n",
    "f = requests.get(s_url,headers = headers)\n",
    "# 获取搜索到的前40条数据\n",
    "for i in range(40):\n",
    "  html = requests.get(s_url+'&page='+str(i),headers = headers)\n",
    "  html = html.text\n",
    "  html =urllib.parse.unquote(html)\n",
    "  with open(filepath+'/'+keyword+'.txt','a',encoding =\"utf-8\") as f:\n",
    "     f.write(html)\n",
    "  time.sleep(0.5)\n",
    "#打开该文件  \n",
    "with open(filepath+'/'+keyword+'.txt','r',encoding =\"utf-8\") as h:\n",
    "     html = h.read()\n",
    "#解析内容并下载          \n",
    "uids = re.findall('<a href=\"//weibo.com/(.*?)?refer_flag=1001030103_\" class=\".*?\" target=\".*?\" nick-name=\"(.*?)\" suda-data=\".*?\">.*?</a>',html)\n",
    "contents = re.findall(' <p class=\"txt\" node-type=\"feed_list_content\" nick-name=\".*?\">(.*?)</p>',html,re.S)\n",
    "pic_id = re.findall('<!--card-wrap-->(.*?)<!--/card-wrap-->',html,re.S)\n",
    "data=pd.DataFrame(columns = range(3))\n",
    "data.columns=['用户名','用户id','帖子']\n",
    "for i in range(len(uids)):\n",
    "    uid,nickname = uids[i]\n",
    "    data = pd.concat([data, pd.DataFrame.from_records([{ '用户id': uids[i][0],'用户名': uids[i][1], '帖子': re.sub('<.*?>','',contents[i],re.S)}])])\n",
    "print(data)\n",
    "data.to_csv(\"data/搜索结果.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取评论区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def fetchurl(pid,uid,max_id):\n",
    "    url = \"https://weibo.com/ajax/statuses/buildComments\"\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0)',\n",
    "        'Cookie': 'WEIBOCN_WM=3349; MLOGIN=0; loginScene=102003; SUB=_2A25OZUb0DeRhGeFM71cR8SjPyTyIHXVtpmq8rDV6PUJbkdANLUTtkW1NQNrR0hDYvbkyXXHvhSXTK6gLZwPIvqhj; _T_WM=2f61b9622f544de949daf22f40ebcc99'\n",
    "    }\n",
    "    params = {\n",
    "        \"flow\": 0,\n",
    "        \"is_reload\": 1,\n",
    "        \"id\": pid,\n",
    "        \"is_show_bulletin\": 2,\n",
    "        \"is_mix\": 0,\n",
    "        \"max_id\": max_id,\n",
    "        \"count\": 20,\n",
    "        \"uid\": uid,\n",
    "    }\n",
    "    r = requests.get(url,headers=headers,params=params)\n",
    "    return r.json()\n",
    "def parseJson(jsonObj):\n",
    "    data = jsonObj[\"data\"]\n",
    "    max_id = jsonObj[\"max_id\"]\n",
    "    commentData=[]\n",
    "    for item in data:\n",
    "        # 评论id\n",
    "        comment_Id = item[\"id\"]\n",
    "        # 评论内容\n",
    "        content = BeautifulSoup(item[\"text\"], \"html.parser\").text\n",
    "        # 评论时间\n",
    "        created_at = item[\"created_at\"]\n",
    "        # 点赞数\n",
    "        like_counts = item[\"like_counts\"]\n",
    "        # 评论数\n",
    "        total_number = item[\"total_number\"]\n",
    "\n",
    "        # 评论者 id，name，city\n",
    "        user = item[\"user\"]\n",
    "        userID = user[\"id\"]\n",
    "        userName = user[\"screen_name\"]\n",
    "        userCity = user[\"location\"]\n",
    "\n",
    "        dataItem = [comment_Id, created_at, userID, userName, userCity, like_counts, total_number, content]\n",
    "        commentData.append(dataItem)\n",
    "    return commentData,max_id\n",
    "def save_data(data, path, filename):\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    dataframe.to_csv(path + filename, encoding='utf_8_sig', mode='a', index=False, sep=',', header=False )\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pid = 4717939545342043      # 微博id\n",
    "    uid = 6512991534            # 用户id\n",
    "    max_id = 0\n",
    "    path = \"./data/\"           # 保存的路径\n",
    "    filename = \"comments.csv\"   # 保存的文件名\n",
    "\n",
    "    csvHeader = [[\"评论id\", \"发布时间\", \"用户id\", \"昵称\", \"城市\", \"点赞数\", \"回复数\", \"评论内容\"]]\n",
    "    save_data(csvHeader, path, filename)\n",
    "\n",
    "    while(True):\n",
    "        html = fetchurl(pid, uid, max_id)\n",
    "        comments, max_id = parseJson(html)\n",
    "        save_data(comments, path, filename)\n",
    "        # 爬取结束\n",
    "        if max_id == 0:\n",
    "            break;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制评论区ip归属地地图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts.charts import Geo\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/comments.csv')\n",
    "# 建立geo坐标系\n",
    "geo = Geo()\n",
    "attr=list(df[\"城市\"])\n",
    "# value = list(df[\"城市\"].value_counts())\n",
    "\n",
    "# 基础数据\n",
    "city=df[\"城市\"].value_counts()\n",
    "print(city.index.dtype)\n",
    "data=[[\"广东\"],[\"山东\"],[\"河南\"],[\"四川\"],[\"江苏\"],[\"河北\"],[\"湖南\"],[\"安徽\"],[\"浙江\"],[\"湖北\"],[\"广西\"],[\"云南\"],[\"江西\"],[\"辽宁\"],[\"黑龙江\"],[\"陕西\"],[\"山西\"],[\"福建\"],[\"重庆\"],[\"贵州\"],[\"吉林\"],[\"甘肃\"],[\"内蒙古\"],[\"上海\"],[\"台湾\"],[\"新疆\"],[\"北京\"],[\"天津\"],[\"海南\"],[\"香港\"],[\"青海\"],[\"宁夏\"],[\"西藏\"],[\"澳门\"],[\"海外\"]]\n",
    "loc=[]\n",
    "i=0\n",
    "while i<len(data):\n",
    "    j=0\n",
    "    sum=0.0\n",
    "    while j<len(city):\n",
    "        # print(city.index[j].find(data[i][0]))\n",
    "        if city.index[j].find(data[i][0]) !=-1:\n",
    "            sum=sum+city[j]\n",
    "        j+=1\n",
    "    data[i].append(sum)\n",
    "    i+=1\n",
    "i=0\n",
    "while i<len(data):\n",
    "    data[i]=tuple(data[i])\n",
    "    i+=1\n",
    "print(data)\n",
    "\n",
    "# 绘制地图\n",
    "from pyecharts.charts import Map,Geo\n",
    "from pyecharts import options as opts\n",
    "map=(\n",
    "    Map()\n",
    "    .add(\"\",data,\"china\")\n",
    "    .set_global_opts(\n",
    "        title_opts=opts.TitleOpts(title=\"评论区ip归属地地图\",subtitle=\"数据来源：微博评论区IP归属地\",pos_right=\"center\",pos_top=\"5%\"),\n",
    "        visualmap_opts=opts.VisualMapOpts(max_=50),\n",
    "    )  \n",
    ")\n",
    "map.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户表情使用统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
